{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais Artificiais\n",
    "\n",
    "Redes neurais artificiais (RNAs) foram originalmente concebidas em meados do século XX como um modelo computacional do cérebro humano. Sua utilização foi pequena inicialmente devido ao limitado poder computacional disponível na época, além de algumas questões teóricas que somente foram resolvidas em meados da década de 1980. \n",
    "\n",
    "## Introdução\n",
    "\n",
    "Duas décadas após o ressurgimento das RNAs com a criação do algoritmo de retropropagação de erro em 1986, aliado a um crescente numero dados rotulados e o aumento do poder de processamento em placas gráficas se tornou possivel treinar redes neurais profundas (com mais de três camadas) com técnicas que ficaram conhecidas como **Deep Learning**. Um dos exemplos mais famosos de Deep Learning é o artigo [\"Gato do Youtube\"](https://arxiv.org/abs/1112.6209), de Andrew Ng et al. Para outro exemplo mais recente aplicado em robótica veja esse [artigo](https://arxiv.org/abs/1805.03183) e esse [video](https://www.youtube.com/watch?v=B_UgAlsW99s&feature=youtu.be) de Avelino Forechi et al.\n",
    "\n",
    "Esta aula aborda os fundamentos das RNAs, ou seja, redes de uma única camada. Mesmo simples abordaremos três aplicações: regressão linear, classificação entre duas classes usando o algoritmo perceptron e classificação multi-classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histórico\n",
    "\n",
    "O primeiro marco na história das redes neurais artificiais ocorreu com a apresentação de um modelo de neurônio artificial por [McCulloch and Pitts, 1943](https://link.springer.com/article/10.1007/BF02478259). As atividades nessa linha de pesquisa culminaram na concepção do perceptron por [Rosenblatt, 1958](http://dx.doi.org/10.1037/h0042519) e em outro modelo semelhante, o adaline por [Widrow and Hoff, 1960](http://www-isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf). \n",
    "\n",
    "O perceptron é capaz de classificar entre classes que são linearmente separáveis. Foi usado para reconhecer por exemplo caracteres. Essa aplicação foi realizada em uma máquina chamada MARK I PERCEPTRON e causou uma grande euforia certamente exagerada em relação a imaginação das capacidades de futuros robôs inteligentes. \n",
    "\n",
    "A característica importante do perceptron foi a apresentação de um algoritmo de aprendizagem capaz de adaptar os pesos internos do neurônio de maneira que fosse capaz de resolver o problema de classificação linear, em caso da separabilidade linear das classes.\n",
    "\n",
    "O caso exemplar das limitações do perceptron é o problema “Ou exclusivo” (XOR)\n",
    "(f(0,0) = f(1,1) = 0, f(0,1) = f(1,0) = 1) que prova que uma função tão simples de classificação não pode ser calculada pelo perceptron. \n",
    "\n",
    "Essa crítica centrou-se no livro “Perceptrons” de [Minsky and Papert, 1969](https://mitpress.mit.edu/books/perceptrons). O impacto dessa crítica foi tão grande que a comunidade científica abandonou a área das redes neurais artificiais, com a exceção de alguns pesquisadores por exemplo Fukushima, Grossberg, Hopfield e Kohonen.\n",
    "\n",
    "A solução para o problema XOR já era conhecida. Bastava acrescentar mais uma camada de neurônios na rede (uma camada escondida). O que faltava era um algoritmo que fosse capaz de treinar os pesos dessa rede multi-camada para que pudesse classificar corretamente problemas mais complexos. \n",
    "\n",
    "Várias soluções equivalentes foram descobertas durante os anos seguintes, más só a publicação do algoritmo de “retropropagação de erro” (error backpropagation) por [Rumelhart et al., 1986] popularizou uma solução de carácter universal para esse tipo de problema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teoria\n",
    "\n",
    "A terminologia da rede neural é inspirada nas operações biológicas de células chamadas neurônios. Um neurônio é uma célula que possui várias entradas que podem ser ativadas por algum processo externo. Dependendo da quantidade de ativação, o neurônio produz sua própria atividade e envia isto ao longo de suas saídas. Além disso, caminhos de entrada ou saída específicos podem ser \"reforçados\" ou ponderados mais que outros caminhos. \n",
    "\n",
    "A hipótese é que, como o cérebro humano é formado por uma rede de neurônios, podemos emular o cérebro modelando um neurônio e conectando-o por meio de um grafo de conexões com pesos (caminhos ponderados).\n",
    "\n",
    "O equivalente artificial de uma célula neuronal é um nó (também chamado de neurônio artificial) que recebe um conjunto de entradas ponderadas, calcula a soma das entradas ponderadas para ser utilizada pela função de ativação $ \\phi $, que repassa o resultado da função de ativação para os nós mais adiante no grafo. \n",
    "\n",
    "Graficamente, se parece com o seguinte:\n",
    "\n",
    "<img src=\"img/node.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "Observe que é mais simples representar a entrada para nossa função de ativação como um produto escalar:\n",
    "\n",
    "$$ \\phi \\left(\\sum_i w_i a_i\\right) = \\phi(\\mathbf{w}^T\\mathbf a) $$\n",
    "\n",
    "Existem várias funções de ativação: \n",
    "- Podemos usar uma função de ativação linear ou identidade:\n",
    "$$ \\phi (\\mathbf{w}^T \\mathbf{a}) = \\mathbf{w}^T \\mathbf{a} $$\n",
    "\n",
    "- Outro exemplo é a função de ativação sigmóide:\n",
    "$$ \\phi (\\mathbf{w}^T\\mathbf{a}) = \\frac{1} {1+\\exp(-\\mathbf{w}^T \\mathbf{a})} $$\n",
    "\n",
    "- Mais um exemplo é a função de ativação tangente hiperbólica tanh:\n",
    "$$ \\phi (\\mathbf{w}^T\\mathbf{a}) = \\mbox{tanh} (\\mathbf{w}^T\\mathbf{a}) $$\n",
    "\n",
    "Podemos então formar uma rede neural juntando esses nós. Geralmente isso é feito em camadas - as saídas de uma camada de nós são conectadas às próximas entradas da camada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% plote a função sigmoide no intervalo [-10 10]\n",
    "sigmoid = @(x) 1 ./ (1 + exp(-x));\n",
    "ezplot(sigmoid, [-10 10])\n",
    "\n",
    "% plote a tangente hiperbólica no intervalo [-10 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topologia\n",
    "\n",
    "O potencial e flexibilidade do cálculo baseado em redes neurais vêm da criação de conjuntos de neurônios que estão interligados entre si. \n",
    "\n",
    "Esse paralelismo de elementos com processamento local cria a “inteligência” global da rede. Um elemento da rede recebe um estimulo nas suas entradas, processa esse sinal e emite um novo sinal de saída para fora que por sua vez é recebido pelos outros elementos.\n",
    "\n",
    "Uma categorização fundamental da topologia dos neurônios pode ser feita em relação ao método de propagação da informação recebida, veja a figura abaixo. \n",
    "\n",
    "<img src=\"img/topologia.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "Pode-se distinguir entre redes de propagação para frente (feedforward) e redes realimentadas (recurrent). \n",
    "- No caso das redes de propagação para frente o fluxo de informação é unidirecional. \n",
    "- No caso das redes realimentadas não há restrições para estabelecer ligações entre os neurônios. \n",
    "\n",
    "Neurônios que recebem a informação simultaneamente agrupam-se em camadas. Camadas que não estão ligadas às **entradas** e nem às **saídas** da rede chamam-se **camadas ocultas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paradigmas de Aprendizagem\n",
    "\n",
    "Uma vez definida a rede neural, essa tem que ser treinada. Isso significa que os graus de liberdade que a rede dispõe, para solucionar a tarefa em consideração, têm que ser adaptados\n",
    "de uma maneira ótima. \n",
    "\n",
    "Normalmente, isso significa que temos que modificar os pesos $w_{ij}$ entre o neurônio $i$ e o neurônio $j$, segundo um algoritmo. Um conjunto finito de n exemplos de treino está à nossa disposição para adaptar os pesos durante a fase de treinamento da rede.\n",
    "\n",
    "Uma distinção principal em relação ao paradigma de aprendizagem que é válido para todo tipo de sistemas com capacidade de adaptação é aprendizagem **supervisionada** e aprendizagem **não-supervisionada**.\n",
    "\n",
    "### Aprendizagem supervisionada\n",
    "\n",
    "Nosso objetivo é treinar uma rede usando dados rotulados para que possamos fornecer um conjunto de entradas e produzir as saídas apropriadas para dados ainda não rotulados. \n",
    "\n",
    "Podemos fazer isso porque temos tanto a entrada $ \\mathbf{x}_i $ quanto a saída de destino desejada $ y_i $ na forma de pares de dados. \n",
    "\n",
    "O treinamento, nesse caso, envolve o aprendizado dos pesos corretos associados a cada sinapse do neurônio para produzir a saída de destino, dada a entrada. \n",
    "\n",
    "A rede e seus pesos treinados formam uma função (denotada $ h $) que opera nos dados de entrada. \n",
    "\n",
    "Com a rede treinada, podemos fazer previsões considerando qualquer entrada de teste não rotulada.\n",
    "\n",
    "<img src=\"img/training_testing.png\" width=\"50%\" height=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos treinar uma rede neural para realizar regressão ou classificação. Nesta parte, vou mostrar a regressão linear com uma rede de camada única. Redes de classificação e multicamadas serão cobertas posteriormente.\n",
    "\n",
    "## Regressão linear\n",
    "\n",
    "A regressão linear é a forma mais simples de regressão. Modelamos nosso sistema com uma combinação linear de sinapses para produzir uma saída. Isto é,\n",
    "\n",
    "$$ y_i = h(\\mathbf{x}_i,\\mathbf{w}) = \\mathbf{w}^T \\mathbf{x}_i $$\n",
    "\n",
    "Nossa tarefa é, então, encontrar os pesos que fornecem o melhor ajuste aos nossos dados de treinamento. Uma forma de medir nosso ajuste é calcular o erro (ou perda) de mínimos quadrados sobre nosso conjunto de dados:\n",
    "\n",
    "$$ L(\\mathbf{w}) = \\sum_i(e_i)^2 = \\sum_i\\left(h(\\mathbf{x}_i, \\mathbf{w}) - y_i\\right)^2$$\n",
    "\n",
    "Para encontrar a linha de melhor ajuste, devemos minimizar $ L (\\mathbf{w}) $. Que possui uma solução analitica fechada para mínimos quadrados, mas em geral podemos minimizar a perda usando gradiente descendente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino\n",
    "\n",
    "Então, o que isso tem a ver com redes neurais? De fato, a rede neural mais simples que existe faz a regressão por mínimos quadrados. Considere a seguinte rede neural de camada única, com um único nó que usa uma função de ativação linear:\n",
    "\n",
    "<img src=\"img/linear_1layer.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "Essa rede toma como entrada um conjunto de dados com duas sinapses $ x_i^{(1)}, x_i^{(2)} $, pondera as sinapses com $ w_1, w_2 $ e as soma para ao final fazer uma predição. \n",
    "\n",
    "Poderíamos definir uma rede que coleta dados com mais sinapses, mas teríamos que monitorar mais pesos, por exemplo, $ w_1, \\ldots, w_j $ com $ j $ sinapses.\n",
    "\n",
    "Se usarmos a função de erro quadrático para medir o desempenho de nossa rede, ela seria idêntica ao erro definido para a regressão de mínimos quadrados acima:\n",
    "\n",
    "$$ L (\\mathbf{w}) = \\sum_i \\left(h(\\mathbf{x}_i, \\mathbf{w}) - y_i\\right)^2$$\n",
    "\n",
    "Este é o erro quadrático total das previsões da nossa rede sobre todo o nosso conjunto de treinamento em $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, usaremos método de otimização gradiente descendente sobre o gradiente do erro $ \\nabla_ {\\mathbf{w}} L(\\mathbf{w}) $ para minimizar o erro geral nos dados de treinamento. \n",
    "\n",
    "Primeiro derivamos o gradiente do erro em relação a um peso específico $ w_{j \\rightarrow k} $ (que é apenas o peso da sinapse que conecta o nó $ j $ ao nó $ k $ [note que tratamos as entradas como \"nós\" logo existe um peso $ w_{j \\rightarrow k} $ para cada conexão da entrada (sinapse) para um nó da primeira camada]) no caso geral:\n",
    "\n",
    "$$ \\begin{align} \\frac{\\partial}{\\partial w_{j \\rightarrow k}} L(\\mathbf{w}) =& \\frac{\\partial}{\\partial w_{j \\rightarrow k}} \\sum_i \\left(h(\\mathbf{x}_i, \\mathbf{w})-y_i\\right)^2\\\\ =& \\sum_i \\frac{\\partial}{\\partial w_{j \\rightarrow k}} \\left(h(\\mathbf{x}_i, \\mathbf{w})-y_i\\right)^2\\\\ =& \\sum_i 2\\left(h(\\mathbf{x}_i, \\mathbf{w})-y_i\\right) \\frac{\\partial}{\\partial w_{j \\rightarrow k}} h(\\mathbf{x}_i, \\mathbf{w}) \\end{align} $$\n",
    "\n",
    "Neste ponto, devemos calcular o gradiente da função da rede neural em relação ao peso em questão ($ \\frac{\\partial}{\\partial w_{j \\rightarrow k}} h(\\mathbf{x}_i, \\mathbf{w}) $). No caso de uma rede de camada única, isso acaba sendo bem simples.\n",
    "\n",
    "Lembre-se da rede mais simples de duas entradas acima. A função da rede é $ h(\\mathbf{x}_i, \\mathbf{w}) = w_1x_i^{(1)} + w_2x_i^{(2)} $. O gradiente em relação a $ w_1 $ é apenas $ x_1 $ e o gradiente em relação a $ w_2 $ é apenas $ x_2 $. \n",
    "\n",
    "Geralmente armazenamos todos os pesos da nossa rede em um vetor ou matriz, então o gradiente completo é:\n",
    "$$ \\nabla_{\\mathbf{w}}L(\\mathbf{w}) = \\left(\\frac{\\partial L(\\mathbf{w})}{\\partial w_1}, \\frac{\\partial L(\\mathbf{w})}{\\partial w_2}\\right) = \\left(\\sum_i 2x_i^{(1)}\\mathbf{e}_i, \\sum_i 2x_i^{(2)}\\mathbf{e}_i\\right) $$\n",
    "\n",
    "Com isso, nós podemos atualizar os pesos da rede usando o método de descida gradiente padrão:\n",
    "\n",
    "$$ \\mathbf{w} = \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}} L(\\mathbf{w}) $$\n",
    "\n",
    "Assim como em todos os métodos de gradiente descendente, deve-se ter cuidado ao selecionar o tamanho do passo \"correto\" $ \\eta $, onde \"correto\" é dependente do problema. Depois de uma quantidade definida de épocas, os pesos que encontramos definem a reta com o melhor ajuste aos dados treinados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste\n",
    "\n",
    "Com nossa rede treinada, o teste consiste em obter uma previsão para cada conjunto de teste $ x_i $ usando $ h (\\mathbf{x}_i, \\mathbf{w}) $. O erro de teste é calculado com a função de erro quadrático, exatamente como no treinamento:\n",
    "\n",
    "$$ L(\\mathbf{w}) = \\sum_i \\left( h(\\mathbf{x}_i, \\mathbf{w}) - y_i\\right)^2 = \\sum_i \\left( \\hat{y}_i - y_i\\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo\n",
    "\n",
    "Para essa implementação, usaremos o peso de um carro para prever seu desempenho em Milhas por Galão (MPG). Os dados são algo como estes exibidos no gráfico de dispersão:\n",
    "\n",
    "<img src=\"img/mpg.png\" width=\"75%\" height=\"75%\">\n",
    "\n",
    "Observe que essa relação não parece ser linear - a regressão linear provavelmente não encontrará a relação subjacente entre peso e MPG. No entanto, ele encontrará uma linha que modela os dados \"muito bem\".\n",
    "\n",
    "Como em nossas aulas, usarei o Octave e matplotlib (para plotagem). Para começar, vamos primeiro carregar os dados do MPG do mpg.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  393\n"
     ]
    }
   ],
   "source": [
    "% Carrega os dados e cria as matrizes de dados X e Y\n",
    "% Isso cria um vetor de entrada X com uma coluna de uns (bias) e uma coluna de pesos de carros.\n",
    "% O vetor alvo Y é uma coluna de valores MPG para cada carro.\n",
    "\n",
    "X_file = csvread('mpg.csv');\n",
    "N = size(X_file)(1)\n",
    "X = [ones(N,1) X_file(:,5)];\n",
    "Y = X_file(:,1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% Normaliza a entrada\n",
    "X(:, 2) = (X(:, 2)-mean(X(:, 2)))/std(X(:, 2));\n",
    "\n",
    "# Plota os dados\n",
    "plot(X(:, 2), Y, 'kx')\n",
    "xlabel('Peso')\n",
    "ylabel('MPG')\n",
    "title('Gráfico de Dispersão')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Há dois pesos, um para o bias e outro para a entrada\n",
    "w = [0, 0];\n",
    "\n",
    "% Inicie o batch de treinamento usando gradiente descendente, \n",
    "% ele será executado por max_iter épocas e terá uma taxa de aprendizado igual a eta\n",
    "max_iter = 100\n",
    "eta = 1E-3\n",
    "for t = 1:max_iter\n",
    "    %Precisamos fazer uma iteração sobre cada conjunto de dados de uma época\n",
    "    grad_t = [0, 0];\n",
    "    for i = 1:N\n",
    "        x_i = X(i, :);\n",
    "        y_i = Y(i);\n",
    "        # Produto escalar, calcula h(x_i, w)\n",
    "        h = dot(w, x_i);\n",
    "        e = h - y_i;\n",
    "        grad_t += 2*x_i*e;\n",
    "    end\n",
    "    # Atualiza os pesos\n",
    "    w = w - eta*grad_t;\n",
    "end\n",
    "disp(\"Pesos encontrados:\")\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plote os dados e a linha de melhor ajuste\n",
    "tt = linspace(min(X(:, 2)), max(X(:, 2)), 10);\n",
    "bf_line = w(1)+w(2)*tt;\n",
    "plot(X(:, 2), Y, 'kx')\n",
    "hold on\n",
    "plot(tt, bf_line, 'r-')\n",
    "xlabel('Peso')\n",
    "ylabel('MPG')\n",
    "title('Regressão com RNA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que torna a rede neural artificial melhor que a regressão linear simples?\n",
    "\n",
    "Uma rede neural artificial sem camadas ocultas é capaz de resolver problemas de regressão linear, mas assim que você começa a adicionar camadas adicionais, a rede adquire a capacidade de resolver problemas de regressão não linear. A regressão linear pode inclusive ser resolvida por um subconjunto de neurônios de uma rede com arquitetura profunda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação Linear\n",
    "\n",
    "Até então vimos apenas o uso de redes neurais para regressão linear. Mas e se quisermos fazer uma classificação usando uma rede de camada única? \n",
    "\n",
    "Veremos dois métodos: o algoritmo do perceptron e o uso de uma função de ativação sigmóide para gerar uma medida de verossimilhança. \n",
    "\n",
    "## Teoria\n",
    "\n",
    "## Perceptron de Camada Única\n",
    "\n",
    "Talvez a rede neural mais simples que podemos definir para classificação binária seja o perceptron de camada única. Dada uma entrada, o neurônio de saída é acionado (produz uma saída igual a 1) somente se o ponto de dados pertencer à classe de destino. Caso contrário, ele não dispara (produz uma saída igual a -1). A rede é algo assim:\n",
    "\n",
    "<img src=\"img/sl_perceptron.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "Em vez de usar uma função de ativação linear como na regressão linear, usamos uma função de sinal. Lembre-se da definição da função de sinal:\n",
    "\n",
    "$$ \\mbox{sign}(\\mathbf{w}^T\\mathbf{x}_i) = \\begin{cases} 1 &\\mbox{if }\\mathbf{w}^T\\mathbf{x}_i > 0 \\\\ 0 &\\mbox{if }\\mathbf{w}^T\\mathbf{x}_i = 0 \\\\ -1 &\\mbox{if }\\mathbf{w}^T\\mathbf{x}_i < 0 \\end{cases} $$\n",
    "\n",
    "Nesta, estamos calculando o produto escalar de uma amostra de dados com nosso vetor de pesos. Pontos com projeções positivas receberão um rótulo 1 e pontos com projeções negativas receberão um rótulo -1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequentemente, nosso limite de decisão será perpendicular ao nosso vetor de peso. Por quê? Considere um problema de decisão bidimensional. O limite de decisão é a linha em que é igualmente provável que qualquer ponto nesta linha pertença a qualquer das classes, ou seja, $ h(\\mathbf{x}_i, \\mathbf{w}) = \\mbox{sign}(\\mathbf{w}^T\\mathbf{x}_i) = 0 $, or $ \\mathbf{w}^T\\mathbf{x}_i = 0 $. Assim temos:\n",
    "\n",
    "$$ \\begin{align} \\mathbf{w}^T\\mathbf{x}_i =& 0\\\\ w_1+w_2x_i^{(2)} + w_3x_i^{(3)} =& 0\\\\ x_i^{(3)} =& -\\frac{w_2}{w_3}x_i^{(2)} -\\frac{w_1}{w_3} \\end{align} $$\n",
    "\n",
    "Neste caso, $ x_i^{(1)} $ é omitido pois o bias é sempre igual a 1, $ x_i ^ {(2)} $ é \"$ x $\" no plano cartesiano e $ x_i ^ {(3)} $ é \"$ y $\". \n",
    "\n",
    "A inclinação do nosso vetor de peso no plano cartesiano é $ \\frac{w_3}{w_2} $ (o componente \"$ y $\" de $ \\mathbf{w} $ é $ w_3 $ e o componente \"$ x $\" é $ w_2 $), enquanto a inclinação do limite de decisão é $ - \\frac{w_2}{w_3} $ (tornando-os perpendiculares). \n",
    "\n",
    "Graficamente, temos:\n",
    "\n",
    "<img src=\"img/linear_db.png\" width=\"25%\" height=\"25%\">\n",
    "\n",
    "O problema que enfrentamos agora é que a função de sinal não é continuamente diferenciável, e não podemos usar a descida de gradiente padrão para aprender os pesos. Portanto, usaremos o algoritmo perceptron propriamente dito. \n",
    "\n",
    "Esse algoritmo é um método iterativo usado para atualizar sucessivamente os pesos que definem um limite linear se e somente se esse limite não classificar um ponto de treinamento corretamente. O algoritmo é o seguinte:\n",
    "\n",
    "Inicialize o vetor de pesos $ \\mathbf{w} $ para todos com zeros.\n",
    "Repita o seguinte:\n",
    "1. Para cada exemplo de treinamento $ \\mathbf{x}_i $:\n",
    "    - Se $ h (\\mathbf {x}_i, \\mathbf {w}) \\neq y_i $, então atualize os pesos com $ \\mathbf {w} '= \\mathbf {w} + \\eta y_i \\mathbf{x}_i $  (aqui, $ \\eta $ é o tamanho do passo).\n",
    "2. Se a condição de parada $ \\frac {1} {N} \\sum_{j = 0} ^ M | w_j '- w_j | <\\delta $ é atingido, então aceite $ \\mathbf {w} $ como o vetor de peso final ($ M $ nesse caso é o número de dimensões de entrada).\n",
    "\n",
    "Se o nosso problema é linearmente separável, o algoritmo do perceptron é garantido para convergir. Portanto, ao termino do algoritmo, acabaremos com um limite de decisão linear definido por $ \\mathbf {w} $.\n",
    "\n",
    "Finalmente, se quisermos predizer o rótulo $ \\hat {y}_i $ de um exemplo de teste $ \\mathbf {x}_i $, usamos $ \\hat {y}_i = \\mbox {sign} (\\mathbf {w } ^ T \\mathbf {x}_i) $.\n",
    "\n",
    "Para implementar essa teoria, treinaremos um conjunto de pesos que classificam dois grupos de dados 2D usando o algoritmo perceptron. Vamos começar definindo nossos dados 2D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two random clusters of 2D data\n",
    "N_c = 100\n",
    "A = 0.3*randn(N_c, 2)+[1, 1];\n",
    "B = 0.3*randn(N_c, 2)+[3, 3];\n",
    "X = [ones(2*N_c, 1), vertcat(A, B)];\n",
    "Y = [-1*ones(N_c, 1), ones(N_c, 1)];\n",
    "N = 2*N_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse código gera dois agrupamentos 2D centralizados em torno dos pontos (1, 1) e (3, 3). Em seguida, acrescenta 1 a cada ponto, que é o nosso bias. Por fim, ele atribui o rótulo 1 a um agrupamento e o rótulo -1 ao outro agrupamento. Os exemplos são colocados na matriz $ X $ e os rótulos no vetor $ Y $.\n",
    "\n",
    "Em seguida, executamos o algoritmo do perceptron para aprender os pesos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo perceptron\n",
    "delta = 1E-7;\n",
    "eta = 1E-2;\n",
    "max_iter = 500;\n",
    "w = [0, 0, 0];\n",
    "w_old = [0, 0, 0];\n",
    "for t = 1:max_iter\n",
    "    for i = 1:N\n",
    "        x_i = X(i, :);\n",
    "        y_i = Y(i);\n",
    "        h = sign(dot(w, x_i));\n",
    "        if h ~= y_i\n",
    "            w = w+eta*y_i*x_i;\n",
    "        end\n",
    "    end\n",
    "    if (1/N)*abs(sum(w_old-w)) < delta\n",
    "        fprintf (\"Converged in %d steps.\", t);\n",
    "        break\n",
    "    end\n",
    "    w_old = w;\n",
    "\n",
    "    if t==max_iter\n",
    "        disp (\"Warning, did not converge.\");\n",
    "    end\n",
    "end\n",
    "disp (\"Weights found:\");\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trecho segue o algoritmo do perceptron diretamente. Na linha 5, inicializamos os pesos em 0. Na linha 6, também inicializamos um vetor que armazena os pesos anteriores, que usaremos para testar a condição de parada. \n",
    "\n",
    "Observe que executamos o algoritmo um numero maximo de iterações max_iter para que ele não seja executado indefinidamente se o problema não for separável.\n",
    "\n",
    "Na linha 12, nós testamos para ver se $ h (\\mathbf {x}_i) = y_i $, e se não atualizamos os pesos de acordo. \n",
    "\n",
    "Finalmente, testamos a condição de parada na linha 16.\n",
    "\n",
    "Em um teste em particular, você pode ver que o algoritmo convergiu em 6 etapas:\n",
    "\n",
    "<img src=\"img/two_class.gif\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "E se, em vez de usar o perceptron, quiséssemos aprender uma sigmoide usando gradiente descendente? Podemos usar o mesmo procedimento para descida de gradiente, conforme detalhado na regressão linear. No entanto, fizemos inicialmente a descida de gradiente por uma quantidade definida de épocas.\n",
    "\n",
    "Aqui nós introduzimos uma condição alternativa de parada. A heurística é a seguinte: se a norma do gradiente for pequena, devemos estar próximos do mínimo da função. Assim, se definirmos um limite para a norma do gradiente e ele ficar abaixo desse limite em uma etapa, paramos. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação com uma função de ativação sigmóide (softmax)\n",
    "\n",
    "Em vez de um classificador do tipo \"tudo ou nada\" (como a função de sinal), é útil encontrar alguma forma de medir a probabilidade de atribuição, ou seja, $ P( Y = y_i\\ |\\ X = \\mathbf{x}_i, \\mathbf{w} ) $. Se pudermos calcular essa probabilidade, podemos usar como uma medida de confiança de nossas previsões.\n",
    "\n",
    "Em vez de usar uma função de ativação de sinal, podemos usar uma sigmóide (geralmente chamado softmax na literatura de rede neural) para gerar uma probabilidade:\n",
    "\n",
    "$$ P(Y = y\\ |\\ X = \\mathbf{x}_i, \\mathbf{w}) = \\frac{1}{1+\\exp\\left(-y\\mathbf{w}^T\\mathbf{x}_i\\right)}$$\n",
    "\n",
    "Mas como podemos atribuir um rótulo de uma classe quando dado apenas uma probabilidade? Podemos simplesmente \"limitar\" a probabilidade usando uma função de sinal, de modo que qualquer $ P(Y = 1\\ |\\ \\mathbf{x}_i, \\mathbf{w}) \\geq 0.5$ receba um rótulo de classe 1, e qualquer probabilidade menor que 0,5 recebe um rótulo de classe -1. Nossa rede simples agora se parece com o seguinte:\n",
    "\n",
    "<img src=\"img/linear_sigmoid.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "Felizmente, essa função é idêntica à probabilidade usada pela regressão logística. Como a sigmoide é diferenciável, podemos usar gradiente padrão para treinar os pesos em vez do algoritmo do perceptron. Para uma derivação do gradiente para regressão logística, veja abaixo.\n",
    "\n",
    "A probabilidade de nossos dados definidos por regressão logística é: $$ P(D_Y | D_X, \\mathbf{w}) = \\prod_i^N \\frac{1}{1+\\exp(-y_i\\mathbf{w}^T\\mathbf{x_i})} $$ \n",
    "\n",
    "Então a probabilidade do log é: $$ \\ell(\\mathbf{w}) = \\log(P(D_Y | D_X, \\mathbf{w})) = -\\sum_i^N \\log(1+\\exp(-y_i\\mathbf{w}^T\\mathbf{x}_i) $$ \n",
    "\n",
    "Para a subida do gradiente, estamos tentando selecionar $ \\mathbf{w} $ para maximizar a probabilidade do log. \n",
    "\n",
    "Para fazer isso, devemos primeiro calcular o gradiente: $$\\begin{align} \\nabla_{\\mathbf{w}}\\ell(\\mathbf{w}) &= \\frac{\\partial}{\\partial \\mathbf{w}} \\left(-\\sum_i^N \\log(1+\\exp(-y_i\\mathbf{w}^T\\mathbf{x}_i))\\right)\\\\ &= \\sum_i^N \\frac{y_i\\mathbf{x}_i\\exp(-y_i\\mathbf{w}^T\\mathbf{x_i})}{1+\\exp(-y_i\\mathbf{w}^T\\mathbf{x}_i)} \\end{align}$$ \n",
    "\n",
    "Porque $$ 1 - P(y_i | \\mathbf{x}_i, \\mathbf{w}) = \\frac{\\exp(-y_i\\mathbf{w}^T\\mathbf{x_i})}{1+\\exp(-y_i\\mathbf{w}^T\\mathbf{x}_i)} $$ o gradiente é $$ \\nabla_{\\mathbf{w}}\\ell(\\mathbf{w}) = \\sum_i^N y_i\\mathbf{x}_i(1-P(y_i | \\mathbf{x}_i, \\mathbf{w})) $$ \n",
    "\n",
    "Para evitar ter que mudar a taxa de aprendizado para diferentes tamanhos de conjuntos de dados, podemos dividir a taxa pelo número de exemplos: $$ \\nabla_{\\mathbf{w}}\\ell(\\mathbf{w}) = \\frac{1}{N}\\sum_i^N y_i\\mathbf{x}_i(1-P(y_i | \\mathbf{x}_i, \\mathbf{w})) $$\n",
    "\n",
    "Para implementar essa teoria, estaremos aprendendo um conjunto de pesos que classificam dois grupos de dados 2D usando o gradiente descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent\n",
    "delta = 1E-7;\n",
    "eta = 1E-3;\n",
    "max_iter = 1000;\n",
    "w = [0, 0, 0]\n",
    "grad_thresh = 5;\n",
    "for t = 1:max_iter\n",
    "    grad_t = [0., 0., 0.];\n",
    "    for i = 1:N\n",
    "        x_i = X(i, :);\n",
    "        y_i = Y(i);\n",
    "\n",
    "        grad_t += y_i*x_i*(exp(-y_i*dot(w, x_i)))/(1+exp(-y_i*dot(w, x_i)));\n",
    "    end\n",
    "    w = w + (1/N)*eta*grad_t;\n",
    "    grad_norm = norm(grad_t,2);\n",
    "    if grad_norm < grad_thresh\n",
    "        fprintf( \"Converged in %d steps.\", t);\n",
    "        break\n",
    "    end\n",
    "end\n",
    "disp (\"Weights found:\")\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na linha 13, calculamos o gradiente de acordo com a derivação da sigmoide. Nas linhas 16-17, verificamos se chegamos à condição de parada. Esta é uma heurística simples que pode não funcionar em todos os casos, mas funciona bem o suficiente para um problema simples como este."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
